{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ghckd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(filepath_or_buffer=\"../data/train.csv\")\n",
    "test = pd.read_csv(filepath_or_buffer=\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"keyword\"] == np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def text_cleaner(text:str):\n",
    "        if text is not np.nan:\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "            stop = stopwords.words(\"english\")\n",
    "            text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def keyword_splitter(x:str):\n",
    "        if \"%20\" in x:\n",
    "            x = tuple(x.split(\"%20\"))\n",
    "        else:\n",
    "            x = (x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    for col in [\"location\", \"text\"]:\n",
    "        data[col] = data[col].apply(text_cleaner)    \n",
    "        \n",
    "    # data[\"keyword\"] = data[\"keyword\"].fillna(\"\")\n",
    "    # data[\"keyword_split\"] = data[\"keyword\"].apply(keyword_splitter)\n",
    "\n",
    "    # data[\"location\"] = data[\"location\"].fillna(\"\")\n",
    "    data[\"text\"] = data[\"text\"].fillna(np.nan)\n",
    "    \n",
    "    data = data.drop(columns=[\"id\", \"keyword\", \"location\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train[\"text\"].tolist()), \n",
    "                                  specials=['<UNK>'],                    # 스페셜 토큰\n",
    "                                  min_freq=2,                            # 최소 빈도 토큰\n",
    "                                  max_tokens=5000\n",
    "                                  )\n",
    "vocab.set_default_index(vocab['<UNK>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, valid_set = train_test_split(\n",
    "    train,\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "valid_set = valid_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 5329\n",
      "검증 샘플의 개수 : 2284\n",
      "테스트 샘플의 개수 : 3263\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(train_set)))\n",
    "print('검증 샘플의 개수 : {}'.format(len(valid_set)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 y의 비율 : 0.4362919872396322\n",
      "검증 샘플의 y의 비율 : 0.4141856392294221\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 샘플의 y의 비율 :\", train_set[\"target\"].sum()/len(train_set))\n",
    "print(\"검증 샘플의 y의 비율 :\", valid_set[\"target\"].sum()/len(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, vocab, tokenizer, text, labels=None):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        label = self.labels[index]\n",
    "        return self.vocab(self.tokenizer(text)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(vocab, tokenizer, train_set[\"text\"], train_set[\"target\"])\n",
    "valid_set = CustomDataset(vocab, tokenizer, valid_set[\"text\"], valid_set[\"target\"])\n",
    "test_set = CustomDataset(vocab, tokenizer, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, max_sequence_length):\n",
    "    label_list, text_list = [], []\n",
    "    \n",
    "    for text, label in batch:\n",
    "        processed_text = torch.tensor(text[:max_sequence_length], dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    \n",
    "    text_list = pad_sequence([torch.tensor(text) for text in text_list], \n",
    "                             batch_first=True, \n",
    "                             )\n",
    "    \n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=lambda x: collate_batch(x, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "valid_loader = DataLoader(valid_set, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False, \n",
    "                          collate_fn=lambda x: collate_batch(x, MAX_SEQUENCE_LENGTH))\n",
    "test_loader = DataLoader(test_set, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          collate_fn=lambda x: collate_batch(x, MAX_SEQUENCE_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '__dataframe__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__dataframe__\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute '__dataframe__'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def build_vocab(data, max_vocab_size):\n",
    "#     counter = Counter()\n",
    "#     for text in data:\n",
    "#         counter.update(text.split())\n",
    "#     vocab = {token: index + 2 for index, (token, count) in enumerate(counter.most_common(max_vocab_size))}\n",
    "#     vocab[\"<unk>\"] = 0\n",
    "#     vocab[\"<pad>\"] = 1\n",
    "    \n",
    "#     return vocab\n",
    "\n",
    "# vocab = build_vocab(train[\"text\"], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text, vocab):\n",
    "#     return [vocab.get(token, vocab[\"<unk>\"]) for token in text.split()]\n",
    "\n",
    "# def collate_fn(samples):\n",
    "#     if isinstance(samples[0], tuple):\n",
    "#         texts, labels = zip(*samples)\n",
    "#         tokenized_texts = [tokenize(text, vocab) for text in texts]\n",
    "#         padded_texts = pad_sequence(\n",
    "#             [torch.tensor(text) for text in tokenized_texts], \n",
    "#             batch_first=True\n",
    "#             )\n",
    "#         return padded_texts, torch.tensor(labels)\n",
    "#     else:\n",
    "#         tokenized_texts = [tokenize(text, vocab) for text in samples]\n",
    "#         padded_texts = pad_sequence(\n",
    "#             [torch.tensor(text) for text in tokenized_texts], \n",
    "#             batch_first=True\n",
    "#             )\n",
    "#         return padded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     , \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn\n",
    "#     )\n",
    "# valid_loader = DataLoader(\n",
    "#     val_set, \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn\n",
    "#     )\n",
    "# test_loader = DataLoader(\n",
    "#     test_set.data[\"text\"], \n",
    "#     batch_size=batch_size,\n",
    "#     collate_fn=collate_fn\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    # preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim, \n",
    "        hidden_size,\n",
    "        output_size, \n",
    "        dropout):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, bidirectional=False)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(64, output_size), \n",
    "            nn.Sigmoid()      \n",
    "        )\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        hidden = output[:, -1, :]\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=output_size, \n",
    "    dropout=dropout\n",
    "    )\n",
    "\n",
    "criterion = nn.BCELoss()  ##nn.BCEWithLogitsLoss() ##  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch\n",
    "        predictions = model(text).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            \n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            total_loss += loss.item() * text.size(0)\n",
    "            \n",
    "            preds = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (preds == labels).float()\n",
    "            total_correct += correct.sum().item()\n",
    "            \n",
    "            total_samples += text.size(0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples \n",
    "    \n",
    "    return avg_loss, accuracy           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghckd\\AppData\\Local\\Temp\\ipykernel_4300\\1501677118.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_list = pad_sequence([torch.tensor(text) for text in text_list],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.689 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 41.42%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.687 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 41.42%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.684 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 41.42%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.686 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 41.42%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.684 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 41.42%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.682 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 41.42%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.678 | Train Acc: 0.02%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 41.42%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.681 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 41.42%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.684 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 41.42%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.680 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.680 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 41.42%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 41.42%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.680 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 41.42%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.678 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 41.42%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.678 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.04%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.46%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.02%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 41.46%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.06%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 41.46%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.675 | Train Acc: 0.11%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 41.46%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.683 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 41.42%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 41.42%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 41.42%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.02%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.04%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 41.42%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.681 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.06%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 41.42%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.06%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.02%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 41.42%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.04%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.680 | Train Acc: 0.06%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.675 | Train Acc: 0.11%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 41.42%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.678 | Train Acc: 0.09%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 41.42%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.13%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 41.42%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.04%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 41.42%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.675 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.682 | Train Acc: 0.04%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 41.42%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.675 | Train Acc: 0.11%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.09%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 41.42%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.11%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 41.42%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.679 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 41.42%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.675 | Train Acc: 0.02%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.676 | Train Acc: 0.13%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 41.42%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.677 | Train Acc: 0.07%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 41.42%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    # scheduler.step(valid_loss)\n",
    "    \n",
    "    # # Early stopping\n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     early_stopping_counter = 0\n",
    "    # else:\n",
    "    #     early_stopping_counter += 1\n",
    "    #     if early_stopping_counter >= patience:\n",
    "    #         print(\"Early stopping triggered!\")\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def predict(model, test_loader):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():  # Disable gradient tracking during inference\n",
    "#         for texts, labels in test_loader:\n",
    "#             output = model(texts).squeeze(1)\n",
    "#             print(output)\n",
    "#             probabilities = torch.sigmoid(output)  # Apply sigmoid to obtain probabilities\n",
    "#             predictions.extend(torch.round(probabilities).cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     return predictions, true_labels\n",
    "\n",
    "# # Assuming you have a test_loader\n",
    "# test_predictions, true_labels = predict(model, test_loader)\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, test_predictions)\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster-tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
